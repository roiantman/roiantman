{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentinement_Analysis_logistic_nlp_techtrain_todo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roiantman/roiantman/blob/main/Sentinement_Analysis_logistic_nlp_techtrain_todo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using logistic regression\n",
        "\n",
        "In this notebook, you will implement a logistic regression model for sentiment analysis on tweets. You will classify each tweet as positive sentiment or a negative one. \n"
      ],
      "metadata": {
        "id": "PuUd0EeuG7e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text pre-processing methods"
      ],
      "metadata": {
        "id": "d4VtkrBQK6He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and tokanization\n",
        "As we saw in the presentation , Tokenization is the process of breaking down the given text into the smallest unit in a sentence called a token.\n",
        "Let's look at some examples, we will use \n"
      ],
      "metadata": {
        "id": "D-cdK7dUpN49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is an example for our tokenizer. this is great !\"\n",
        "sent_tokens=nltk.sent_tokenize(text)\n",
        "words_tokens = nltk.word_tokenize(text)\n",
        "print(\"sent_tokens:\",sent_tokens)\n",
        "print(\"words_tokens:\",words_tokens)"
      ],
      "metadata": {
        "id": "6BdsYOuYGjGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StopWords \n",
        "During this step, all words that are not actually conveying information are removed. Examples of such words: \"the\", \"and\", \"it\".These words are generally common in every corpus, and don’t give any information on the sentiment of the text. \n",
        "\n",
        "In most cases, we can use NLTK’s stop words list `stopwords.words()` , but we might want to add words to this list depending on the context of our problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "70yTbzmiPPX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "DBq2DO5p4H4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words(\"english\")[:20]"
      ],
      "metadata": {
        "id": "sFAqOCsT45zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sometimes we also want to remove punctioation , we can use `string.punctuation`to get list of all punctuation."
      ],
      "metadata": {
        "id": "Uyxd3wuWHA_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "8fFVO6k7G8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming \n",
        "Stemming is the process of finding the root of words. To apply stemming we cut the suffixes in words according to a certain rule.\n",
        "\n",
        "By applying ruled-based stemming we have two optional problems : overstemming and understemming.\n",
        "\n",
        "* **Overstemming** occurs when words are over-truncated. In such cases, the meaning of the word may be distorted or have no meaning.\n",
        "\n",
        "* **Understemming** occurs when two words are stemmed from the same root that is not of different stems.\n",
        "\n",
        "To apply stemming we will use Porter Stemmer , if you are interested in more information you can read more about it  here :[Porter Stemming algorithm](https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/)"
      ],
      "metadata": {
        "id": "hk9bAQCx7PHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "word = (\"tokenization\")\n",
        "ps.stem(word)"
      ],
      "metadata": {
        "id": "DQKw-1L67YHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "We will use The “Twitter Sentiment Analysis” dataset on Kaggle.\n",
        "is a collection of approximately 74,000 tweets, we will use a sample of the dataset to save time in the course."
      ],
      "metadata": {
        "id": "VgCEmN6QKQnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "id": "i6BPbcp6GxHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's see an example of how our "
      ],
      "metadata": {
        "id": "7TxNN321LLMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_twitters=pd.read_csv(\"/content/twitter_training.csv\")\n",
        "train_twitters.rename(columns = {'Tweet Content':'Tweet'}, inplace = True)\n",
        "\n",
        "train_twitters.head(10)"
      ],
      "metadata": {
        "id": "5aoglwKUHusv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested only in the Tweet Content and it's sentiment."
      ],
      "metadata": {
        "id": "iPAF-GjEPSu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters=train_twitters[[\"Sentiment\",\"Tweet\"]]"
      ],
      "metadata": {
        "id": "p_CC5wurPcji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters.Sentiment.value_counts()"
      ],
      "metadata": {
        "id": "BFjW4sxspDlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "train_twitters=train_twitters.drop_duplicates().reset_index(drop=True)\n",
        "train_twitters"
      ],
      "metadata": {
        "id": "BtQmbQ6HpQ73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters=train_twitters[train_twitters[\"Sentiment\"]!=\"Irrelevant\"].reset_index(drop=True)\n",
        "train_twitters"
      ],
      "metadata": {
        "id": "Z7rwlCnIp_F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, since it is a supervised learning we want to map each \"Positive\" sentiment to \"1\" , \"Neutral\" to 0 and and \"Negative\" sentiment to -1 ."
      ],
      "metadata": {
        "id": "F3rbdSzwPpd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters['Sentiment']=train_twitters['Sentiment'].map({\"Positive\":1,\"Neutral\":0,\"Negative\":-1})\n",
        "train_twitters"
      ],
      "metadata": {
        "id": "PYu5nSFZPo3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there is NaN value \n",
        "train_twitters.isna().any()"
      ],
      "metadata": {
        "id": "F3KFjB7CFC9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop Nan\n",
        "train_twitters=train_twitters.dropna()"
      ],
      "metadata": {
        "id": "kzWCW9QBtiVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters"
      ],
      "metadata": {
        "id": "ymwSqzuG1nL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we sample to get a subset of data , for saving time so we can learn more stuff ! :)\n",
        "train_twitters = train_twitters.sample(10000,random_state=0)"
      ],
      "metadata": {
        "id": "tLiyA7Cj0vkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters=train_twitters.reset_index(drop=True)\n",
        "train_twitters"
      ],
      "metadata": {
        "id": "Gl9Sktoy1kLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x= 'Sentiment',data = train_twitters)"
      ],
      "metadata": {
        "id": "DkeINxQE5jk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is pretty much balanced- we have about the same number of \"Positive\" and \"Negative\" samples ."
      ],
      "metadata": {
        "id": "t5bwHaExTcaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Removing hyperlinks, twitter marks, styles:\n",
        "Twitter consists many substrings like hashtag, retweet marks, hyperlinks, which do not participate in sentiment analysis. Removing these substrings is carried through re(regular expression) library in Python."
      ],
      "metadata": {
        "id": "eSQYLNH5e43W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hyperlinks(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet=re.sub(r'pic.twitter.com/\\S+', '', tweet)\n",
        "    tweet=re.sub(r'twitter.com/\\S+', '', tweet)\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
        "    tweet = tweet.strip('[link]') # remove [links]\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "zIqw0oM-qogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_twitter_styles(tweet):\n",
        "  # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet=remove_hyperlinks(tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "LdkDmMFJam0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtA8Hsk7EOmV"
      },
      "outputs": [],
      "source": [
        "from nltk import TweetTokenizer\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: for each tweet we retuen a list of clean,proccesed words from the tweet\n",
        "    \"\"\"\n",
        "    tweet=clean_twitter_styles(tweet)\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    \n",
        "    # TODO - tokenize tweets - you can use nltk special tokenizer for tweet TweetTokenizer\n",
        "    tweet_tokens = ...\n",
        "\n",
        "    process_tweet = []\n",
        "    # TODO - we want for each word in the tweet_tokens - remove stopwords, and punctuation\n",
        "    # for removing punctiuation you can \n",
        "    \n",
        "    #  TODO - now for each word we want to apply stemming \n",
        "    \n",
        "    return process_tweet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to apply our function on the Tweet colum to get for each token it's relevant tokens.\n",
        "\n",
        "This may take a few minutes\n"
      ],
      "metadata": {
        "id": "oFdpgVwzHsM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters['Tweet_tokens']=train_twitters[\"Tweet\"].apply(process_tweet)"
      ],
      "metadata": {
        "id": "1TckKLzUvq7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_twitters"
      ],
      "metadata": {
        "id": "IN3VgtlQwNPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build our model \n",
        "We will use a simple logistic regressiong - DO I need to add explanation ?"
      ],
      "metadata": {
        "id": "oacwOwz-H5mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data -\n",
        "We will use 75% from the data for training and 25% for validation"
      ],
      "metadata": {
        "id": "_C6-JGJ8IGqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = train_twitters['Tweet'] \n",
        "y = train_twitters['Sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state = 0)"
      ],
      "metadata": {
        "id": "tTgsratDoyuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train.reset_index(drop=True)\n",
        "y_train=y_train.reset_index(drop=True)\n",
        "X_test=X_test.reset_index(drop=True)\n",
        "y_test=y_test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "K3DOPU5ftDAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize words \n",
        "To fit our model we need to represent our words as numeric representation . \n",
        "We will use Bag of Word as our representation"
      ],
      "metadata": {
        "id": "_KvgvGtqIbNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(ngram_range=(1,2)) \n",
        "# create BOW\n",
        "X_train_bow = count_vect.fit_transform(X_train) \n",
        "X_test_bow = count_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "g5AZ-4HXrXXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "# normalize bow for better performance \n",
        "X_train_bow = preprocessing.normalize(X_train_bow)\n",
        "X_test_bow = preprocessing.normalize(X_test_bow)"
      ],
      "metadata": {
        "id": "yZwIOiucuKX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose model \n",
        "we will use a simple LogisticRegression \n",
        "We chose using the solver and max iteration are hyper parameters. You can change them and fine\n"
      ],
      "metadata": {
        "id": "sNp39cA7I6GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model= LogisticRegression(solver='lbfgs', max_iter=1000)"
      ],
      "metadata": {
        "id": "b0gcu1X0uWMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To train our model we use model.fit(features,label)\n",
        "model.fit(X_train_bow,y_train)"
      ],
      "metadata": {
        "id": "9DnXYLBt80Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evluate our model"
      ],
      "metadata": {
        "id": "f_K_RZpnJqOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_train = model.predict(X_train_bow)\n",
        "pred_test = model.predict(X_test_bow)"
      ],
      "metadata": {
        "id": "YnZmsKqZ7FmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm_bow = confusion_matrix(y_test, pred_test)"
      ],
      "metadata": {
        "id": "yJuu8-vX8oal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=cm_bow, figsize=(6, 6))\n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rfd2t_Qw9bIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_train=accuracy_score(y_train, pred_train)\n",
        "accuracy_train"
      ],
      "metadata": {
        "id": "vXvLAI6Q9pQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_train=accuracy_score(y_test, pred_test)\n",
        "accuracy_train"
      ],
      "metadata": {
        "id": "7ZF7j-cMBYV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}